{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b544f348",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e48d8",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e1893",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{{\\bf h}}^{(t)}=&{\\bf W}_{{\\bf x}{\\bf h}}{\\bf x}^{(t)}+{\\bf W}_{{\\bf h}{\\bf h}}{\\bf h}^{(t-1)}+ {\\bf b}_{\\bf h}\\\\\n",
    "{\\bf h}^{(t)} =& \\tanh\\left(\\bar{{\\bf h}}^{(t)}\\right)\\\\\n",
    "\\bar{{\\bf o}}^{(t)}=&{\\bf W}_{{\\bf h}{\\bf o}}{\\bf h}^{(t)}+{\\bf b}_{\\bf o}\\\\\n",
    "{\\bf o}^{(t)} =& f\\left(\\bar{{\\bf o}}^{(t)}\\right)\\\\\n",
    "\\mathcal{L}=&\\sum_{t=1}^T\\sum_{i=1}^d -y^{(t)}_i \\log\\left(o^{(t)}_i\\right)\\\\\n",
    "=&\\sum_{t=1}^T-\\log\\left(o^{(t)}_{i_*^{(t)}}\\right)\\\\\n",
    "\\text{where }&{i_*^{(t)}}=\\arg\\max_i\\{y_i^{(t)}\\}\\text{ with }y_{i_*^{(t)}}^{(t)}=1 \\\\\n",
    "&{\\bf x}^{(t)}\\in \\mathbb{R}^d,{\\bf h}^{(t)}\\in \\mathbb{R}^m, {\\bf o}^{(t)}\\in \\mathbb{R}^d\\\\\n",
    "&{\\bf W}_{{\\bf x}{\\bf h}}\\in\\mathbb{R}^{m\\times d},{\\bf W}_{{\\bf h}{\\bf h}}\\in\\mathbb{R}^{m\\times m},{\\bf W}_{{\\bf h}{\\bf o}}\\in \\mathbb{R}^{d\\times m},{\\bf b}_{\\bf h}\\in \\mathbb{R}^m,{\\bf b}_{\\bf o}\\in \\mathbb{R}^d\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\bar{o}^{(t)}_j}&=\\frac{\\partial \\mathcal{L}}{\\partial {o}^{(t)}_{i_*^{(t)}}}\\frac{\\partial {o}^{(t)}_{i_*^{(t)}}}{\\partial \\bar{o}^{(t)}_j}=-\\frac{1}{o_{i_{*}^{(t)}}^{(t)}}\\frac{\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}e^{\\bar{o}^{(t)}_{i_*^{(t)}}}\\left(\\sum \\cdot\\right)-e^{\\bar{o}^{(t)}_{i_*^{(t)}}}e^{\\bar{o}^{(t)}_{j}}}{\\left(\\sum \\cdot\\right)^{2}}=-\\frac{1}{o_{i_{*}^{(t)}}^{(t)}}\\left(\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}o_{i_{*}^{(t)}}^{(t)}-o_{i_{*}^{(t)}}^{(t)}o_{j}^{(t)}\\right)=o_{j}^{(t)}-\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf b}_{\\bf o}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf b}_{\\bf o}}=\\sum_{t=1}^{T}I^\\top\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right)=\\sum_{t=1}^{T}{\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf h}{\\bf o}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf W}_{{\\bf h}{\\bf o}}}=\\sum_{t=1}^{T}\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right){{\\bf h}^{(t)}}^\\top\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}=\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t+1)}}\\frac{\\partial {\\bf h}^{(t+1)}}{\\partial {\\bf h}^{(t)}}+\\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf h}^{(t)}}={\\bf W}_{{\\bf h}{\\bf h}}^\\top\\text{diag}\\left({\\bf 1}-\\left({{\\bf h}^{(t+1)}}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t+1)}}+{\\bf W}_{{\\bf h}{\\bf o}}^\\top\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right)\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf h}{\\bf h}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\frac{\\partial {\\bf h}^{(t)}}{\\partial {\\bf W}_{{\\bf h}{\\bf h}}}=\\sum_{t=1}^{T} \\text{diag}\\left({\\bf 1}-\\left({\\bf h}^{(t)}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\left({\\bf h}^{(t-1)}\\right)^\\top\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf x}{\\bf h}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\frac{\\partial {\\bf h}^{(t)}}{\\partial {\\bf W}_{{\\bf x}{\\bf h}}}=\\sum_{t=1}^{T} \\text{diag}\\left({\\bf 1}-\\left({\\bf h}^{(t)}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\left({\\bf x}^{(t)}\\right)^\\top\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c39445",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96f40ae-299f-4893-8d16-7822215ce2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "323139b9-fe23-403d-9cb7-f0a8ce38e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    def __init__(self, fpath):\n",
    "        self._fpath = fpath\n",
    "        with open(self._fpath, 'r') as f:\n",
    "            self.data = f.read()\n",
    "            self.data = self.data.replace(\"\\n\", ' ')\n",
    "            self.data = self.data.lower()\n",
    "        self.data = re.sub(r'([\\\"\\'])?([A-z]+[-\\']?[A-z]+)?([\\\"\\':,.?!])?', r'\\1 \\2 \\3', self.data).split()\n",
    "        chars = list(set(self.data))\n",
    "        self.char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "        self.vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e6dcab6e-5bce-4b94-bd63-f013dfee0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size, learning_rate=0.1, sequence_length=25, hidden_size=100):\n",
    "        self._learning_rate = learning_rate # learning rate\n",
    "        self._sequence_length = sequence_length\n",
    "        self._hidden_size = hidden_size\n",
    "        self._vocab_size = vocab_size\n",
    "        np.random.seed(42)\n",
    "        self._Wxh = np.random.randn(self._hidden_size,self._vocab_size)*0.01\n",
    "        self._Whh = np.random.randn(self._hidden_size,self._hidden_size)*0.01\n",
    "        self._Who = np.random.randn(self._vocab_size,self._hidden_size)*0.01\n",
    "        self._bh = np.zeros(hidden_size)\n",
    "        self._bo = np.zeros(vocab_size)\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        ex = np.exp(x-max(x))\n",
    "        return ex/np.sum(ex)\n",
    "        \n",
    "    def loss(self, os, y):\n",
    "        return sum(-np.log(max(os[t][y[t]],0.1/self._vocab_size)) for t in range(self._sequence_length))\n",
    "        # mind the \\\n",
    "        # return loss using Xentropy\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os = {}, {}, {}\n",
    "        hs[-1] = hprev\n",
    "        for t in range(self._sequence_length):\n",
    "            xs[t] = np.zeros(self._vocab_size)\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(self._Wxh@xs[t]+self._Whh@hs[t-1]+self._bh)\n",
    "            os[t] = self._softmax(self._Who@hs[t]+self._bo)\n",
    "        return xs, hs, os\n",
    "\n",
    "    \n",
    "    def backward(self, xs, hs, os, y):\n",
    "        dbo, dbh = np.zeros_like(self._bo), np.zeros_like(self._bh)\n",
    "        dWho, dWhh, dWxh = np.zeros_like(self._Who), np.zeros_like(self._Whh), np.zeros_like(self._Wxh)\n",
    "        dhnext = np.zeros(self._hidden_size)\n",
    "        for t in range(self._sequence_length-1,-1,-1):\n",
    "            do = np.zeros(self._vocab_size)+os[t]\n",
    "            do[y[t]] -= 1\n",
    "            dh = (1-hs[t]**2)*(self._Who.T@do+dhnext)\n",
    "            dhnext = self._Whh.T@dh\n",
    "            dbo += do\n",
    "            dbh += dh\n",
    "            dWho += np.outer(do,hs[t])\n",
    "            dWhh += np.outer(dh,hs[t-1])\n",
    "            dWxh += np.outer(dh,xs[t])\n",
    "        for dparam in (dbo, dbh, dWho, dWhh, dWxh):\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dbo, dbh, dWho, dWhh, dWxh\n",
    "    \n",
    "    def fit(self, reader, epoch):\n",
    "        smooth_loss = -np.log(1.0/self._vocab_size)*self._sequence_length\n",
    "        mbo, mbh = np.zeros_like(self._bo), np.zeros_like(self._bh)\n",
    "        mWho, mWhh, mWxh = np.zeros_like(self._Who), np.zeros_like(self._Whh), np.zeros_like(self._Wxh)\n",
    "        \n",
    "        for _ in tqdm(range(epoch)): \n",
    "            n = 0\n",
    "            hprev = np.zeros(self._hidden_size)\n",
    "            for p in range(0, len(reader.data)-self._sequence_length, self._sequence_length):\n",
    "                inputs = [reader.char_to_ix[ch] for ch in reader.data[p:p+self._sequence_length]]\n",
    "                #print(inputs)\n",
    "                y = inputs[1:]+[reader.char_to_ix[reader.data[p+self._sequence_length]]]\n",
    "                #print(y)\n",
    "                xs, hs, os = self.forward(inputs, hprev)\n",
    "                dbo, dbh, dWho, dWhh, dWxh = self.backward(xs, hs, os, y)\n",
    "                #print(dWho)\n",
    "                for param, dparam, mem in zip((self._bo, self._bh, self._Who, self._Whh, self._Wxh),\n",
    "                                              (dbo, dbh, dWho, dWhh, dWxh),\n",
    "                                              (mbo, mbh, mWho, mWhh, mWxh)):\n",
    "                    mem += dparam*dparam\n",
    "                    param += -self._learning_rate*dparam/np.sqrt(mem+1e-8)\n",
    "                hprev = hs[self._sequence_length-1]\n",
    "                smooth_loss = smooth_loss * 0.999 + self.loss(os, y) * 0.001\n",
    "                n+=1\n",
    "                if n%400 == 0:\n",
    "                    print(f\"loss: {smooth_loss}\")\n",
    "                    print(self.predict(reader,'I', self._sequence_length))\n",
    "\n",
    "    def predict(self, reader, start, length):\n",
    "        start = start.lower()\n",
    "        start_ix = reader.char_to_ix[start]\n",
    "        x = np.zeros(self._vocab_size)\n",
    "        x[start_ix] = 1\n",
    "        ixes = [start_ix]\n",
    "        hprev = np.zeros(self._hidden_size)\n",
    "        for t in range(length):\n",
    "            hprev = np.tanh(self._Wxh@x+self._Whh@hprev+self._bh) \n",
    "            o = self._softmax(self._Who@hprev+self._bo)\n",
    "            ix = np.random.choice(range(self._vocab_size), p=o)\n",
    "            ixes.append(ix)\n",
    "            x = np.zeros(self._vocab_size)\n",
    "            x[ix] = 1\n",
    "        text = ' '.join(reader.ix_to_char[ix] for ix in ixes)\n",
    "        text = re.sub(R'([\\\"\\']) ([A-z]+[-\\']?[A-z]+) ([\\\"\\'])', r'\\1\\2\\3', text)\n",
    "        text = re.sub(R'([A-z\\'\\\"]) ([:,.?!])', r'\\1\\2', text)\n",
    "        return text.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c1871825-6e02-40d0-a407-8a6a1f1bbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fpath = 'shakespeare.txt'\n",
    "reader = Reader(fpath)\n",
    "print(f'{reader.vocab_size} unique words')\n",
    "rnn = RNN(vocab_size=reader.vocab_size,hidden_size=500)\n",
    "rnn.fit(reader=reader, epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34deba",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "de3fd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, vocab_size, hidden_size=100, sequence_length=25, learning_rate=0.01, beta1=0.9, beta2=0.999):\n",
    "        self._hidden_size = hidden_size\n",
    "        self._vocab_size = vocab_size\n",
    "        self._sequence_length = sequence_length \n",
    "        self._learning_rate = learning_rate\n",
    "        self._beta1 = beta1 # momentum parameter in Adam\n",
    "        self._beta2 = beta2 # rmsprop parameter in Adam\n",
    "        self._params = {}\n",
    "        # glorot initialization \n",
    "        std = 1/np.sqrt(self._hidden_size+self._vocab_size)\n",
    "        # forget gate\n",
    "        self._params['Wf'] = np.random.randn(self._hidden_size, self._hidden_size+self._vocab_size) * std\n",
    "        self._params['bf'] = np.zeros((self._hidden_size,1))\n",
    "        # input gate\n",
    "        self._params['Wi'] = np.random.randn(self._hidden_size, self._hidden_size+self._vocab_size) * std\n",
    "        self._params['bi'] = np.zeros((self._hidden_size,1))\n",
    "        # cell gate\n",
    "        self._params['Wc'] = np.random.randn(self._hidden_size, self._hidden_size+self._vocab_size) * std\n",
    "        self._params['bc'] = np.zeros((self._hidden_size,1))\n",
    "        # output gate\n",
    "        self._params['Wo'] = np.random.randn(self._hidden_size, self._hidden_size+self._vocab_size) * std\n",
    "        self._params['bo'] = np.zeros((self._hidden_size,1))\n",
    "        # output\n",
    "        self._params['Wy'] = np.random.randn(self._vocab_size, self._hidden_size) / np.sqrt(self._hidden_size)\n",
    "        self._params['by'] = np.zeros((self._vocab_size,1))\n",
    "    \n",
    "    @property\n",
    "    def _Wf(self): return self._params['Wf']\n",
    "    @property\n",
    "    def _bf(self): return self._params['bf']\n",
    "    @property\n",
    "    def _Wi(self): return self._params['Wi']\n",
    "    @property\n",
    "    def _bi(self): return self._params['bi']\n",
    "    @property\n",
    "    def _Wc(self): return self._params['Wc']\n",
    "    @property\n",
    "    def _bc(self): return self._params['bc']\n",
    "    @property\n",
    "    def _Wo(self): return self._params['Wo']\n",
    "    @property\n",
    "    def _bo(self): return self._params['bo']\n",
    "    @property\n",
    "    def _Wy(self): return self._params['Wy']\n",
    "    @property\n",
    "    def _by(self): return self._params['by']\n",
    "    \n",
    "    def loss(self, Yhat, y):\n",
    "        return sum(-np.log(max((Yhat[t].ravel())[y[t]],0.1/self._vocab_size)) for t in range(self._sequence_length))\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        ex = np.exp(x-max(x))\n",
    "        return ex/np.sum(ex)\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def forward(self, inputs, hprev, cprev):\n",
    "        X, Z, F, I, Cbar, C, O, H, Yhat = {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "        H[-1] = hprev\n",
    "        C[-1] = cprev\n",
    "        for t in range(self._sequence_length):\n",
    "            X[t] = np.zeros((self._vocab_size,1))\n",
    "            X[t][inputs[t]] = 1\n",
    "            Z[t] = np.row_stack((H[-1], X[t]))\n",
    "            F[t] = self._sigmoid(self._Wf@Z[t]+self._bf)\n",
    "            I[t] = self._sigmoid(self._Wi@Z[t]+self._bi)\n",
    "            Cbar[t] = np.tanh(self._Wc@Z[t]+self._bc)\n",
    "            C[t] = F[t]*C[t-1] + I[t]*Cbar[t]\n",
    "            O[t] = self._sigmoid(self._Wo@Z[t]+self._bo)\n",
    "            H[t] = O[t]*np.tanh(C[t])\n",
    "            Yhat[t] = self._softmax(self._Wy@H[t]+self._by)\n",
    "        return Z, F, I, Cbar, C, O, H, Yhat\n",
    "                \n",
    "    def backward(self, Z, F, I, Cbar, C, O, H, Yhat, y):\n",
    "        dparams = {key:np.zeros_like(self._params[key]) for key in self._params.keys()}\n",
    "        dhnext = np.zeros((self._hidden_size,1))\n",
    "        dcnext = np.zeros((self._hidden_size,1))\n",
    "        for t in range(self._sequence_length-1, -1, -1):\n",
    "            dy = np.zeros((self._vocab_size,1))+Yhat[t]\n",
    "            dy[y[t]] -= 1\n",
    "            dh = self._Wy.T@dy + dhnext\n",
    "            dc = dh*O[t]*(1-np.tanh(C[t])**2) +dcnext\n",
    "            # output\n",
    "            dparams['Wy'] += dy@H[t].T\n",
    "            dparams['by'] += dy\n",
    "            # forget gate\n",
    "            df = dc*C[t-1]*(F[t]-F[t]**2)\n",
    "            dparams['Wf'] += df@Z[t].T\n",
    "            dparams['bf'] += df\n",
    "            # input gate\n",
    "            di = dc*Cbar[t]*(I[t]-I[t]**2)\n",
    "            dparams['Wi'] += di@Z[t].T\n",
    "            dparams['bi'] += di\n",
    "            # cell gate\n",
    "            dcbar = dc*I[t]*(1-Cbar[t]**2)\n",
    "            dparams['Wc'] += dcbar@Z[t].T\n",
    "            dparams['bc'] += dcbar\n",
    "            # output gate\n",
    "            do = dh*np.tanh(C[t])*(O[t]-O[t]**2)\n",
    "            dparams['Wo'] += do@Z[t].T\n",
    "            dparams['bo'] += do\n",
    "            # update for next iteration\n",
    "            dhnext = (self._Wf.T@df + self._Wi.T@di + self._Wc.T@dc + self._Wo.T@do)[:self._hidden_size, :]\n",
    "            dcnext = dc*F[t]\n",
    "        for dparam in dparams.values():\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dparams\n",
    "    \n",
    "    def fit(self, reader, epoch, verbose=True):\n",
    "        smooth_loss = -np.log(1.0/self._vocab_size)*self._sequence_length\n",
    "        vparams = {key:np.zeros_like(self._params[key]) for key in self._params.keys()}\n",
    "        sparams = {key:np.zeros_like(self._params[key]) for key in self._params.keys()}\n",
    "        n = 1\n",
    "        for _ in tqdm(range(epoch)):\n",
    "            hprev = np.zeros((self._hidden_size,1))\n",
    "            cprev = np.zeros((self._hidden_size,1))\n",
    "            for p in range(0, len(reader.data)-self._sequence_length, self._sequence_length):\n",
    "                inputs = [reader.char_to_ix[ch] for ch in reader.data[p:p+self._sequence_length]]\n",
    "                y = inputs[1:]+[reader.char_to_ix[reader.data[p+self._sequence_length]]]\n",
    "                if verbose:\n",
    "                    if n%100 == 0:\n",
    "                        print(f\"loss: {smooth_loss}\")\n",
    "                        print(self.predict(reader,reader.data[p], self._sequence_length, hprev, cprev))\n",
    "                Z, F, I, Cbar, C, O, H, Yhat = self.forward(inputs, hprev, cprev)\n",
    "                dparams = self.backward(Z, F, I, Cbar, C, O, H, Yhat, y)\n",
    "                for key in self._params.keys():\n",
    "                    vparams[key] = self._beta1*vparams[key]+(1-self._beta1)*dparams[key]\n",
    "                    sparams[key] = self._beta2*sparams[key]+(1-self._beta2)*dparams[key]**2\n",
    "                    v_corrected = vparams[key]/(1-self._beta1**n)\n",
    "                    s_corrected = sparams[key]/(1-self._beta2**n)\n",
    "                    self._params[key] -= self._learning_rate*v_corrected/(np.sqrt(s_corrected)+1e-8)\n",
    "                hprev = H[self._sequence_length-1]\n",
    "                cprev = C[self._sequence_length-1]\n",
    "                smooth_loss = smooth_loss*0.999 + self.loss(Yhat, y)*0.001\n",
    "                n += 1    \n",
    "                \n",
    "    def predict(self, reader, start, length, hprev=np.array([None]), cprev=np.array([None])):\n",
    "        start = start.lower()\n",
    "        start_ix = reader.char_to_ix[start]\n",
    "        hprev = hprev.copy() if hprev.all() else np.zero((self._hidden_size,1))\n",
    "        cprev = cprev.copy() if cprev.all() else np.zero((self._hidden_size,1))\n",
    "        x = np.zeros((self._vocab_size,1))\n",
    "        x[start_ix] = 1\n",
    "        ixes = [start_ix]\n",
    "        for t in range(length):\n",
    "            z = np.row_stack((hprev, x))\n",
    "            f = self._sigmoid(self._Wf@z+self._bf)\n",
    "            i = self._sigmoid(self._Wi@z+self._bi)\n",
    "            cbar = np.tanh(self._Wc@z+self._bc)\n",
    "            cprev = f*cprev + i*cbar\n",
    "            c = cprev\n",
    "            o = self._sigmoid(self._Wo@z+self._bo)\n",
    "            hprev = o*np.tanh(c)\n",
    "            h = hprev\n",
    "            yhat = self._softmax(self._Wy@h+self._by)\n",
    "            ix = np.random.choice(range(self._vocab_size), p=yhat.ravel())\n",
    "            ixes.append(ix)\n",
    "            x = np.zeros((self._vocab_size,1))\n",
    "            x[ix] = 1\n",
    "        text = ' '.join(reader.ix_to_char[ix] for ix in ixes)\n",
    "        text = re.sub(R'([\\\"\\']) ([A-z]+[-\\']?[A-z]+) ([\\\"\\'])', r'\\1\\2\\3', text)\n",
    "        text = re.sub(R'([A-z\\'\\\"]) ([:,.?!])', r'\\1\\2', text)\n",
    "        return text.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "49fa5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fpath = 'shakespeare.txt'\n",
    "reader = Reader(fpath)\n",
    "print(f'{reader.vocab_size} unique words')\n",
    "lstm = LSTM(vocab_size=reader.vocab_size)\n",
    "lstm.fit(reader=reader, epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd4007",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb730c",
   "metadata": {},
   "source": [
    "https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6516a",
   "metadata": {},
   "source": [
    "Classical Gradient Boost:\n",
    "Idea: creates an ensemble model from numerous weak predictors usually “Regression Trees” which are added in a stage wise fashion with each new tree focusing on the errors of the previous tree to convert these weak learners into a single strong predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30c8dd1",
   "metadata": {},
   "source": [
    "Understanding Gradient Boosting as a gradient descent: http://nicolas-hug.com/blog/gradient_boosting_descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e511e29-c3b9-4140-aa7b-86ac076e78f4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}=\\sum_{i}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_{i}}=\\frac{\\partial}{\\partial \\hat{y}_{i}}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=-2\\left(y_{i}-\\hat{y}_{i}\\right)\\\\\n",
    "\\begin{aligned}\n",
    "\\hat{y}_{i}^{(i+1)} &=\\hat{y}_{i}^{(i)}-\\alpha \\frac{\\partial f}{\\partial \\hat{y}_{i}} \\\\\n",
    "&=\\hat{y}_{i}^{(i)}+2 \\alpha\\left(y_{i}-\\hat{y}_{i}^{(i)}\\right) \\\\\n",
    "&=2 \\alpha y_{i}+(1-2 \\alpha) \\hat{y}_{i}^{(i)}\n",
    "\\end{aligned}\\\\\n",
    "\\hat{y}^{(i+1)}-y_{i}=(1-2 \\alpha)\\left(\\hat{y}_{i}^{(i)}-y_{i}\\right)\\\\\n",
    "y_{i}^{(N)}-y_{i}=(1+2 \\alpha)^{N}\\left(\\hat{y}^{(0)}-y_{i}\\right) \\rightarrow 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b1b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0c851c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.arange(0,10,0.1)\n",
    "ε = np.random.randn(len(X))\n",
    "y = 5*np.sin(X)+9 + ε\n",
    "\n",
    "learning_rate = 0.1\n",
    "yhat = np.zeros_like(y) # initial guess\n",
    "loss = []\n",
    "loss.append(sum((y-yhat)**2))\n",
    "\n",
    "# learning\n",
    "for _ in range(100):\n",
    "    yhat -= -2*(y-yhat)*learning_rate\n",
    "    loss.append(sum((y-yhat)**2))\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf18bd68",
   "metadata": {},
   "source": [
    "In practice, however, $y_i$ is known, we estimate the gradient by tree models in each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8319334c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L} &=\\sum_{i}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\\\\n",
    "&=\\sum_{j} \\sum_{i \\in S_{j}}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\\\\n",
    "&=\\sum_{j} \\sum_{i \\in S_j}\\left(y_{i}-\\bar{y}_{j}\\right)^{2} \\\\\n",
    "&=\\sum_{i}\\left(\\sum_{i \\in S_{j}} y_{i}^{2}-\\left|S_{j}\\right| \\bar{y}_{j}^{2}\\right) \\\\\n",
    "&=\\sum_{i} y_{i}^{2}-\\sum_{j} \\frac{\\left(\\sum_{i \\in S_j}y_{i}\\right)^2}{n_{j}}\\\\\n",
    "&=c-\\sum_{j}\\frac{G_j^2}{n_j}\n",
    "\\end{aligned}\\\\\n",
    "\\min \\mathcal{L} \\Leftrightarrow \\min-\\sum_j\\frac{G_j^2}{n_j}\\\\\n",
    "\\begin{aligned}\n",
    "{\\text {Gain}} &=-\\Delta \\mathcal{L}=-\\left[\\left(-\\frac{G_{j L}^{2}}{n_{j L}}\\right)+\\left(-\\frac{G_{j R}^{2}}{n_{j R}}\\right)-\\left(-\\frac{G_{j}^{2}}{n_{j}}\\right)\\right] \\\\\n",
    "&=\\frac{G_{j L}^{2}}{n_{j L}}+\\frac{G_{j R}^{2}}{n_{j R}}-\\frac{\\left(G_{j L}+G_{j R}\\right)^{2}}{n_{j L}+n_{j R}}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f44439e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T07:50:00.111750Z",
     "start_time": "2021-12-01T07:50:00.085514Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, score, maxdepth):\n",
    "        self.maxdepth = maxdepth \n",
    "        self.score = 0 # gain for split\n",
    "        self.feature = None # split feature\n",
    "        self.value = None # split point\n",
    "        self.idx = None # data index\n",
    "        self.left = None # left child\n",
    "        self.right = None # right child\n",
    "        \n",
    "class DecisionTree:\n",
    "    def __init__(self, minleaf=5, maxdepth=10):\n",
    "        self.minleaf = minleaf\n",
    "        self.maxdepth = maxdepth\n",
    "        self.root = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "    \n",
    "    def gain(self, lidx, ridx):\n",
    "        gl = np.sum(self.y[lidx])\n",
    "        gr = np.sum(self.y[ridx])\n",
    "        nl = len(lidx)\n",
    "        nr = len(ridx)\n",
    "        gain = gl**2/nl+gr**2/nr-(gl+gr)**2/(nl+nr)\n",
    "        return gain \n",
    "        \n",
    "\n",
    "    def split(self, node):\n",
    "        if (node.maxdepth==0) or (len(node.idx)<2*self.minleaf):\n",
    "            return\n",
    "        for feature in range(self.X.shape[1]):\n",
    "            for i in node.idx:\n",
    "                # calculate score\n",
    "                value = self.X[i,feature]\n",
    "                lidx = np.nonzero(self.X[:,feature]<=value)[0][node.idx]\n",
    "                ridx = np.nonzero(self.X[:,feature]> value)[0][node.idx]\n",
    "                if (len(lidx)<self.minleaf) or (len(ridx)<self.minleaf): continue\n",
    "                score = self.gain(lidx,ridx)\n",
    "                if score > node.score:\n",
    "                    node.feature = feature\n",
    "                    node.value = value\n",
    "                    node.score = score\n",
    "        if node.score:\n",
    "            node.left = Node(idx=lidx, maxdepth=node.depth-1)\n",
    "            node.right = Node(idx=ridx, maxdepth=node.depth-1)\n",
    "            self.split(node.left)\n",
    "            self.split(node.right)\n",
    "    \n",
    "    def fit(X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.root = Node(idx=np.arange(self.X.shape[0]),maxdepth=self.maxdepth)\n",
    "        self.split(self.root)\n",
    "        return self\n",
    "        \n",
    "    def predict(X):\n",
    "        y = []\n",
    "        for i in range(X.shape[0]):\n",
    "            node = self.root\n",
    "            while True:\n",
    "                if node.score == 0:\n",
    "                    y.append(np.mean(self.y[node.idx]))\n",
    "                    break\n",
    "                if x[i, node.feature] <= node.value:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "        return np.array(y)\n",
    "    \n",
    "class GraidentBoostingClassifier:\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "        \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def negraderiv(self, y, logodds):\n",
    "        return y-self.sigmoid(logodds)\n",
    "        \n",
    "    def fit(self, X, y, maxdepth=5, minleaf=5, lr=0.1, rounds=5):\n",
    "        self.lr = lr\n",
    "        self.init = np.ones_like(y)*np.log(np.count_nonzero(y==1)/np.count_nonzero(y==0))\n",
    "        self.pred = self.init\n",
    "        for _ in range(rounds):\n",
    "            residual = self.negraderiv(y, self.pred)\n",
    "            gbt = DecisionTree(maxdepth=maxdepth, minleaf=minleaf).fit(X, residual)\n",
    "            self.pred += self.lr*gbt.predict(X)\n",
    "            self.estimators.append(gbt)\n",
    "    \n",
    "    def pred(self, X):\n",
    "        pred = self.init\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.lr*estimator.predict(X)\n",
    "        return pred        \n",
    "        \n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "        \n",
    "    def negraderiv(self, y, yhat): # negative gradient derivative\n",
    "        return 2*(y-yhat)\n",
    "    \n",
    "    def fit(self, X, y, lr=0.1, **kwargs):\n",
    "        self.lr = lr\n",
    "        self.init = np.ones_like(y)*np.mean(y)\n",
    "        self.pred = self.init\n",
    "        for _ in range(rounds):\n",
    "            residual = self.negraderiv(y, self.pred)\n",
    "            gbt = DecisionTree(**kwargs).fit(X, residual)\n",
    "            self.pred += self.lr*gbt.predict(X)\n",
    "            self.estimators.append(gbt)\n",
    "            \n",
    "    def pred(self, X):\n",
    "        pred = self.init\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.lr*estimator.predict(X)\n",
    "        return pred       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c39539",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L} &=-\\sum_{i}\\left[y_{i} \\log p_{i}+\\left(1+y_{i}\\right) \\log \\left(1-p_{i}\\right)\\right] \\\\\n",
    "&=-\\sum_{i}\\left[y_{i} \\log \\left(\\frac{1}{1+e^{-f_{i}}}\\right)+\\left(1-y_{i}\\right) \\log \\left(\\frac{e^{-f_{i}}}{1+e^{-f_{i}}}\\right)\\right]\\\\\n",
    "&=-\\sum_{i}\\left[\\log\\left(\\frac{e^{-f_i}}{1+e^{-f_i}}\\right)-y_i\\left(-f_i\\right)\\right]\\\\\n",
    "&=-\\sum_{i}\\left[y_if_i-\\log\\left(1+e^{f_i}\\right)\\right]\n",
    "\\end{aligned}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial f_{i}}=-y_{i}+\\frac{1}{1+e^{-f i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ec40c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}^{(t)} &=\\ell\\left({\\bf y},\\hat{\\bf y}^{(t-1)}+{\\bf f}^{(t)}\\right)+\\Omega\\left({\\bf f}^{(t)}\\right) \\\\\n",
    "&=\\sum_{j=1}^{m} \\sum_{i \\in S_j}\\left(\\ell_{i}^{(t)}+g_{i}^{(t)} f_{j}^{(t)}+\\frac{1}{2} h_{i}^{(t)} \\left({f_{j}^{(t)}}\\right)^2\\right)+\\left(\\gamma m+\\frac{1}{2} \\lambda \\sum_{j=1}^{m} \\left({f_{j}^{(t)}}\\right)^2\\right)\\\\\n",
    "&=\\sum_{i=1}^{n} \\ell_{i}^{(t)}+\\sum_{j=1}^{m} G_{j}^{(t)} f_{j}^{(t)}+\\frac{1}{2} \\sum_{j=1}^{m}\\left(H_{j}^{(t)}+\\lambda\\right) \\left({f_{j}^{(t)}}\\right)^2+\\gamma m\n",
    "\\end{aligned}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial f_{j}^{(t)}}=\\sum_{j=1}^{m}\\left[G_{j}^{(t)}+\\left(H_{j}^{(t)}+\\lambda\\right) f_{j}^{(t)}\\right]=0\\\\\n",
    "\\Rightarrow f_{j}^{(t)}=-\\frac{G_{j}^{(t)}}{H_{j}^{(t)}+\\lambda}\\\\\n",
    "\\mathcal{L}^{(t)}=\\sum_{i=1}^{n} \\ell_{i}^{(t)}-\\frac{1}{2} \\sum_{j=1}^{m} \\frac{G_{j}^{(t)^{2}}}{H_{j}^{(t)}+\\lambda}+\\gamma m\\\\\n",
    "{\\text {Gain }}_j=\\frac{1}{2}\\left[\\frac{G_{j L}^{2}}{H_{j L}+\\lambda}+\\frac{G_{j R}^{2}}{H_{j R}+\\lambda}-\\frac{\\left(G_{j L}+G_{j R}\\right)^{2}}{H_{j L}+H_{j R}+\\lambda}\\right]-\\gamma\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b420e995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T07:58:44.602155Z",
     "start_time": "2021-12-01T07:58:44.571313Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.score = 0\n",
    "        self.samplecols = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "class XGBoostTree:\n",
    "    def __init__(self, maxdepth=10, minleaf=5, gamma=1, eps=0.01, colsample=0.5, minchildweight=1, lmbda_=1):\n",
    "        self.maxdepth = maxdepth\n",
    "        self.minleaf = minleaf\n",
    "        self.colsample = colsample\n",
    "        self.minchildweight = minchildweight\n",
    "        self.lmbda = lmbda\n",
    "        self.gamma = gamma\n",
    "        self.root = None\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def gain(self, lidx, ridx):\n",
    "        gl = sum(self.grad[lidx])\n",
    "        gr = sum(self.grad[ridx])\n",
    "        hl = sum(self.hess[lidx])\n",
    "        hr = sum(self.hess[ridx])\n",
    "        return 0.5*(gl**2/(hl+self.lmbda)+gr**2/(hr+self.lmbda)-(gl+gr)**2/(hl+hr+self.lmbda))-self.gamma\n",
    "    \n",
    "    @staticmethod\n",
    "    def split(self, node):\n",
    "        if (node.maxdepth==0) or (len(node.idx)<2*self.minleaf):\n",
    "            return\n",
    "        samplecols = np.random.permutation(self.X.shape[1])[:round(self.colsample*self.X.shape[1])]\n",
    "        for feature in samplecols:\n",
    "            for i in node.idx:\n",
    "                value = self.X[i,feature]\n",
    "                lidx = np.nonzero(self.X[:,feature]<=value)[0][node.idx]\n",
    "                ridx = np.nonzero(self.X[:,feature]> value)[0][node.idx]\n",
    "                if (len(lidx)<self.minleaf) \\\n",
    "                or (len(ridx)<self.minleaf) \\\n",
    "                or (sum(self.hess[lidx])<self.minchildweight) \\\n",
    "                or (sum(self.hess[ridx])<self.minchildweight): continue\n",
    "                score = self.gain(lidx,ridx)\n",
    "                if score > node.score:\n",
    "                    node.feature = feature\n",
    "                    node.value = value\n",
    "                    node.score = score\n",
    "        if node.score:\n",
    "            node.samplecols = samplecols\n",
    "            node.left = Node(idx=lidx, maxdepth=node.depth-1)\n",
    "            node.right = Node(idx=ridx, maxdepth=node.depth-1)\n",
    "            self.split(node.left)\n",
    "            self.split(node.right)\n",
    "    \n",
    "    def fit(self, X, grad, hess):\n",
    "        self.X = X\n",
    "        self.grad = grad\n",
    "        self.hess = hess\n",
    "        self.root = Node()\n",
    "        self.split(self.root)\n",
    "        return self\n",
    "    \n",
    "    def predict(X):\n",
    "        y = []\n",
    "        for i in range(X.shape[0]):\n",
    "            node = self.root\n",
    "            while True:\n",
    "                if node.score == 0:\n",
    "                    y.append(-np.sum(self.grad[node.idx])/(np.sum(self.hess[node.idx])+self.lmbda))\n",
    "                    break\n",
    "                if x[i, node.feature] <= node.value:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "        return np.array(y)\n",
    "    \n",
    "    def weighted_quantile_sketch():\n",
    "        ...\n",
    "    \n",
    "class XGBoostClassifier:\n",
    "    def __init__(self, subsample=0.5):\n",
    "        self.subsample = subsample\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "        \n",
    "    def grad(self, y, logodds):\n",
    "        return self.sigmoid(logodds)-y\n",
    "    \n",
    "    def hess(self, y, logodds):\n",
    "        return self.sigmoid(logodds)*(1-self.sigmoid(logodds))\n",
    "        \n",
    "    def fit(self, X, y, lr=0.1, **kwargs):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init = np.ones_like(y)*np.log(np.count_nonzero(y==1)/np.count_nonzero(y==0))\n",
    "        self.pred = self.init\n",
    "        for _ in range(rounds):\n",
    "            sample = np.random.permutation(self.X.shape[0])[:round(self.subsample*self.X.shape[0])]\n",
    "            grad = self.grad(self.y[sample], self.pred[sample])\n",
    "            hess = self.hess(self.y[sample], self.pred[sample])\n",
    "            xgbt = XGBoostTree(**kwargs).fit(self.X[sample], grad, hess)\n",
    "            sel.pred += self.lr*xgbt.predict(self.X)\n",
    "            self.estimators.append(xgbt)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = self.init\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.lr*estimator.predict(X)\n",
    "        return pred\n",
    "    \n",
    "class XGBoostRegressor:\n",
    "    def __init__(self, subsample=0.5):\n",
    "        self.subsample = subsample\n",
    "    \n",
    "    def grad(self, y, yhat):\n",
    "        return 2*(yhat-y)\n",
    "    \n",
    "    def hess(self, y, yhat):\n",
    "        return np.ones_like(y)*2\n",
    "\n",
    "    def fit(self, X, y, lr=0.1, **kwargs):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lr = lr \n",
    "        self.init = np.ones_like(y)*np.mean(y)\n",
    "        self.pred = self.init\n",
    "        for _ in range(rounds):\n",
    "            sample = np.random.permutation(self.X.shape[0])[:round(self.subsample*self.X.shape[0])]\n",
    "            grad = self.grad(self.y[sample], self.pred[sample])\n",
    "            hess = self.hess(self.y[sample], self.pred[sample])\n",
    "            xgbt = XGBoostTree(**kwargs).fit(self.X[sample], grad, hess)\n",
    "            sel.pred += self.lr*xgbt.predict(self.X)\n",
    "            self.estimators.append(xgbt)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = self.init\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.lr*estimator.predict(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53656b51",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:columbia] *",
   "language": "python",
   "name": "conda-env-columbia-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
