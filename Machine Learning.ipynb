{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ac6f2c",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3e984",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ab989f8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{{\\bf h}}^{(t)}=&{\\bf W}_{{\\bf x}{\\bf h}}{\\bf x}^{(t)}+{\\bf W}_{{\\bf h}{\\bf h}}{\\bf h}^{(t-1)}+ {\\bf b}_{\\bf h}\\\\\n",
    "{\\bf h}^{(t)} =& \\tanh\\left(\\bar{{\\bf h}}^{(t)}\\right)\\\\\n",
    "\\bar{{\\bf o}}^{(t)}=&{\\bf W}_{{\\bf h}{\\bf o}}{\\bf h}^{(t)}+{\\bf b}_{\\bf o}\\\\\n",
    "{\\bf o}^{(t)} =& f\\left(\\bar{{\\bf o}}^{(t)}\\right)\\\\\n",
    "\\mathcal{L}=&\\sum_{t=1}^T\\sum_{i=1}^d -y^{(t)}_i \\log\\left(o^{(t)}_i\\right)\\\\\n",
    "=&\\sum_{t=1}^T-\\log\\left(o^{(t)}_{i_*^{(t)}}\\right)\\\\\n",
    "\\text{where }&{i_*^{(t)}}=\\arg\\max_i\\{y_i^{(t)}\\}\\text{ with }y_{i_*^{(t)}}^{(t)}=1 \\\\\n",
    "&{\\bf x}^{(t)}\\in \\mathbb{R}^d,{\\bf h}^{(t)}\\in \\mathbb{R}^m, {\\bf o}^{(t)}\\in \\mathbb{R}^d\\\\\n",
    "&{\\bf W}_{{\\bf x}{\\bf h}}\\in\\mathbb{R}^{m\\times d},{\\bf W}_{{\\bf h}{\\bf h}}\\in\\mathbb{R}^{m\\times m},{\\bf W}_{{\\bf h}{\\bf o}}\\in \\mathbb{R}^{d\\times m},{\\bf b}_{\\bf h}\\in \\mathbb{R}^m,{\\bf b}_{\\bf o}\\in \\mathbb{R}^d\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\bar{o}^{(t)}_j}&=\\frac{\\partial \\mathcal{L}}{\\partial {o}^{(t)}_{i_*^{(t)}}}\\frac{\\partial {o}^{(t)}_{i_*^{(t)}}}{\\partial \\bar{o}^{(t)}_j}=-\\frac{1}{o_{i_{*}^{(t)}}^{(t)}}\\frac{\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}e^{\\bar{o}^{(t)}_{i_*^{(t)}}}\\left(\\sum \\cdot\\right)-e^{\\bar{o}^{(t)}_{i_*^{(t)}}}e^{\\bar{o}^{(t)}_{j}}}{\\left(\\sum \\cdot\\right)^{2}}=-\\frac{1}{o_{i_{*}^{(t)}}^{(t)}}\\left(\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}o_{i_{*}^{(t)}}^{(t)}-o_{i_{*}^{(t)}}^{(t)}o_{j}^{(t)}\\right)=o_{j}^{(t)}-\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf b}_{\\bf o}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf b}_{\\bf o}}=\\sum_{t=1}^{T}I^\\top\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right)=\\sum_{t=1}^{T}{\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf h}{\\bf o}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf W}_{{\\bf h}{\\bf o}}}=\\sum_{t=1}^{T}\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right){{\\bf h}^{(t)}}^\\top\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}=\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t+1)}}\\frac{\\partial {\\bf h}^{(t+1)}}{\\partial {\\bf h}^{(t)}}+\\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf h}^{(t)}}={\\bf W}_{{\\bf h}{\\bf h}}^\\top\\text{diag}\\left({\\bf 1}-\\left({{\\bf h}^{(t+1)}}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t+1)}}+{\\bf W}_{{\\bf h}{\\bf o}}^\\top\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right)\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf h}{\\bf h}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\frac{\\partial {\\bf h}^{(t)}}{\\partial {\\bf W}_{{\\bf h}{\\bf h}}}=\\sum_{t=1}^{T} \\text{diag}\\left({\\bf 1}-\\left({\\bf h}^{(t)}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\left({\\bf h}^{(t-1)}\\right)^\\top\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf x}{\\bf h}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\frac{\\partial {\\bf h}^{(t)}}{\\partial {\\bf W}_{{\\bf x}{\\bf h}}}=\\sum_{t=1}^{T} \\text{diag}\\left({\\bf 1}-\\left({\\bf h}^{(t)}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\left({\\bf x}^{(t)}\\right)^\\top\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182b07b",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f96f40ae-299f-4893-8d16-7822215ce2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "323139b9-fe23-403d-9cb7-f0a8ce38e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    def __init__(self, fpath):\n",
    "        self._fpath = fpath\n",
    "        with open(self._fpath, 'r') as f:\n",
    "            self.data = f.read()\n",
    "            self.data = self.data.replace(\"\\n\", ' ')\n",
    "            self.data = self.data.lower()\n",
    "        self.data = re.sub(r'([\\\"\\'])?([A-z]+[-\\']?[A-z]+)?([\\\"\\':,.?!])?', r'\\1 \\2 \\3', self.data).split()\n",
    "        chars = list(set(self.data))\n",
    "        self.char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "        self.vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e6dcab6e-5bce-4b94-bd63-f013dfee0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size, learning_rate=0.1, sequence_length=25, hidden_size=100):\n",
    "        self._learning_rate = learning_rate # learning rate\n",
    "        self._sequence_length = sequence_length\n",
    "        self._hidden_size = hidden_size\n",
    "        self._vocab_size = vocab_size\n",
    "        np.random.seed(42)\n",
    "        self._Wxh = np.random.randn(self._hidden_size,self._vocab_size)*0.01\n",
    "        self._Whh = np.random.randn(self._hidden_size,self._hidden_size)*0.01\n",
    "        self._Who = np.random.randn(self._vocab_size,self._hidden_size)*0.01\n",
    "        self._bh = np.zeros(hidden_size)\n",
    "        self._bo = np.zeros(vocab_size)\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        ex = np.exp(x-max(x))\n",
    "        return ex/np.sum(ex)\n",
    "        \n",
    "    def loss(self, os, y):\n",
    "        return sum(-np.log(max(os[t][y[t]],0.1/self._vocab_size)) for t in range(self._sequence_length))\n",
    "        # mind the \\\n",
    "        # return loss using Xentropy\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os = {}, {}, {}\n",
    "        hs[-1] = hprev\n",
    "        for t in range(self._sequence_length):\n",
    "            xs[t] = np.zeros(self._vocab_size)\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(self._Wxh@xs[t]+self._Whh@hs[t-1]+self._bh)\n",
    "            os[t] = self._softmax(self._Who@hs[t]+self._bo)\n",
    "        return xs, hs, os\n",
    "\n",
    "    \n",
    "    def backward(self, xs, hs, os, y):\n",
    "        dbo, dbh = np.zeros_like(self._bo), np.zeros_like(self._bh)\n",
    "        dWho, dWhh, dWxh = np.zeros_like(self._Who), np.zeros_like(self._Whh), np.zeros_like(self._Wxh)\n",
    "        dhnext = np.zeros(self._hidden_size)\n",
    "        for t in range(self._sequence_length-1,-1,-1):\n",
    "            do = np.zeros(self._vocab_size)+os[t]\n",
    "            do[y[t]] -= 1\n",
    "            dh = (1-hs[t]**2)*(self._Who.T@do+dhnext)\n",
    "            dhnext = self._Whh.T@dh\n",
    "            dbo += do\n",
    "            dbh += dh\n",
    "            dWho += np.outer(do,hs[t])\n",
    "            dWhh += np.outer(dh,hs[t-1])\n",
    "            dWxh += np.outer(dh,xs[t])\n",
    "        for dparam in (dbo, dbh, dWho, dWhh, dWxh):\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dbo, dbh, dWho, dWhh, dWxh\n",
    "    \n",
    "    def fit(self, reader, epoch):\n",
    "        smooth_loss = -np.log(1.0/self._vocab_size)*self._sequence_length\n",
    "        mbo, mbh = np.zeros_like(self._bo), np.zeros_like(self._bh)\n",
    "        mWho, mWhh, mWxh = np.zeros_like(self._Who), np.zeros_like(self._Whh), np.zeros_like(self._Wxh)\n",
    "        \n",
    "        for _ in tqdm(range(epoch)): \n",
    "            n = 0\n",
    "            hprev = np.zeros(self._hidden_size)\n",
    "            for p in range(0, len(reader.data)-self._sequence_length, self._sequence_length):\n",
    "                inputs = [reader.char_to_ix[ch] for ch in reader.data[p:p+self._sequence_length]]\n",
    "                #print(inputs)\n",
    "                y = inputs[1:]+[reader.char_to_ix[reader.data[p+self._sequence_length]]]\n",
    "                #print(y)\n",
    "                xs, hs, os = self.forward(inputs, hprev)\n",
    "                dbo, dbh, dWho, dWhh, dWxh = self.backward(xs, hs, os, y)\n",
    "                #print(dWho)\n",
    "                for param, dparam, mem in zip((self._bo, self._bh, self._Who, self._Whh, self._Wxh),\n",
    "                                              (dbo, dbh, dWho, dWhh, dWxh),\n",
    "                                              (mbo, mbh, mWho, mWhh, mWxh)):\n",
    "                    mem += dparam*dparam\n",
    "                    param += -self._learning_rate*dparam/np.sqrt(mem+1e-8)\n",
    "                hprev = hs[self._sequence_length-1]\n",
    "                smooth_loss = smooth_loss * 0.999 + self.loss(os, y) * 0.001\n",
    "                n+=1\n",
    "                if n%400 == 0:\n",
    "                    print(f\"loss: {smooth_loss}\")\n",
    "                    print(self.predict(reader,'I', self._sequence_length))\n",
    "\n",
    "    def predict(self, reader, start, length):\n",
    "        start = start.lower()\n",
    "        start_ix = reader.char_to_ix[start]\n",
    "        x = np.zeros(self._vocab_size)\n",
    "        x[start_ix] = 1\n",
    "        ixes = [start_ix]\n",
    "        hprev = np.zeros(self._hidden_size)\n",
    "        for t in range(length):\n",
    "            hprev = np.tanh(self._Wxh@x+self._Whh@hprev+self._bh) \n",
    "            o = self._softmax(self._Who@hprev+self._bo)\n",
    "            ix = np.random.choice(range(self._vocab_size), p=o)\n",
    "            ixes.append(ix)\n",
    "            x = np.zeros(self._vocab_size)\n",
    "            x[ix] = 1\n",
    "        text = ' '.join(reader.ix_to_char[ix] for ix in ixes)\n",
    "        text = re.sub(R'([\\\"\\']) ([A-z]+[-\\']?[A-z]+) ([\\\"\\'])', r'\\1\\2\\3', text)\n",
    "        text = re.sub(R'([A-z\\'\\\"]) ([:,.?!])', r'\\1\\2', text)\n",
    "        return text.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c1871825-6e02-40d0-a407-8a6a1f1bbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fpath = 'shakespeare.txt'\n",
    "reader = Reader(fpath)\n",
    "print(f'{reader.vocab_size} unique words')\n",
    "rnn = RNN(vocab_size=reader.vocab_size,hidden_size=500)\n",
    "rnn.fit(reader=reader, epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451e000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "columbia",
   "language": "python",
   "name": "columbia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
