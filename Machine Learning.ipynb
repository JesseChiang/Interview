{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b544f348",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e48d8",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f0e1893",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{{\\bf h}}^{(t)}=&{\\bf W}_{{\\bf x}{\\bf h}}{\\bf x}^{(t)}+{\\bf W}_{{\\bf h}{\\bf h}}{\\bf h}^{(t-1)}+ {\\bf b}_{\\bf h}\\\\\n",
    "{\\bf h}^{(t)} =& \\tanh\\left(\\bar{{\\bf h}}^{(t)}\\right)\\\\\n",
    "\\bar{{\\bf o}}^{(t)}=&{\\bf W}_{{\\bf h}{\\bf o}}{\\bf h}^{(t)}+{\\bf b}_{\\bf o}\\\\\n",
    "{\\bf o}^{(t)} =& f\\left(\\bar{{\\bf o}}^{(t)}\\right)\\\\\n",
    "\\mathcal{L}=&\\sum_{t=1}^T\\sum_{i=1}^d -y^{(t)}_i \\log\\left(o^{(t)}_i\\right)\\\\\n",
    "=&\\sum_{t=1}^T-\\log\\left(o^{(t)}_{i_*^{(t)}}\\right)\\\\\n",
    "\\text{where }&{i_*^{(t)}}=\\arg\\max_i\\{y_i^{(t)}\\}\\text{ with }y_{i_*^{(t)}}^{(t)}=1 \\\\\n",
    "&{\\bf x}^{(t)}\\in \\mathbb{R}^d,{\\bf h}^{(t)}\\in \\mathbb{R}^m, {\\bf o}^{(t)}\\in \\mathbb{R}^d\\\\\n",
    "&{\\bf W}_{{\\bf x}{\\bf h}}\\in\\mathbb{R}^{m\\times d},{\\bf W}_{{\\bf h}{\\bf h}}\\in\\mathbb{R}^{m\\times m},{\\bf W}_{{\\bf h}{\\bf o}}\\in \\mathbb{R}^{d\\times m},{\\bf b}_{\\bf h}\\in \\mathbb{R}^m,{\\bf b}_{\\bf o}\\in \\mathbb{R}^d\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\bar{o}^{(t)}_j}&=\\frac{\\partial \\mathcal{L}}{\\partial {o}^{(t)}_{i_*^{(t)}}}\\frac{\\partial {o}^{(t)}_{i_*^{(t)}}}{\\partial \\bar{o}^{(t)}_j}=-\\frac{1}{o_{i_{*}^{(t)}}^{(t)}}\\frac{\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}e^{\\bar{o}^{(t)}_{i_*^{(t)}}}\\left(\\sum \\cdot\\right)-e^{\\bar{o}^{(t)}_{i_*^{(t)}}}e^{\\bar{o}^{(t)}_{j}}}{\\left(\\sum \\cdot\\right)^{2}}=-\\frac{1}{o_{i_{*}^{(t)}}^{(t)}}\\left(\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}o_{i_{*}^{(t)}}^{(t)}-o_{i_{*}^{(t)}}^{(t)}o_{j}^{(t)}\\right)=o_{j}^{(t)}-\\mathbb{1}_\\left\\{j=i^{(t)}_*\\right\\}\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf b}_{\\bf o}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf b}_{\\bf o}}=\\sum_{t=1}^{T}I^\\top\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right)=\\sum_{t=1}^{T}{\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf h}{\\bf o}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf W}_{{\\bf h}{\\bf o}}}=\\sum_{t=1}^{T}\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right){{\\bf h}^{(t)}}^\\top\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}=\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t+1)}}\\frac{\\partial {\\bf h}^{(t+1)}}{\\partial {\\bf h}^{(t)}}+\\frac{\\partial \\mathcal{L}}{\\partial \\bar{\\bf o}^{(t)}}\\frac{\\partial \\bar{\\bf o}^{(t)}}{\\partial {\\bf h}^{(t)}}={\\bf W}_{{\\bf h}{\\bf h}}^\\top\\text{diag}\\left({\\bf 1}-\\left({{\\bf h}^{(t+1)}}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t+1)}}+{\\bf W}_{{\\bf h}{\\bf o}}^\\top\\left({\\bf o}^{(t)}-{\\bf e}_{i^{(t)}_*}\\right)\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf h}{\\bf h}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\frac{\\partial {\\bf h}^{(t)}}{\\partial {\\bf W}_{{\\bf h}{\\bf h}}}=\\sum_{t=1}^{T} \\text{diag}\\left({\\bf 1}-\\left({\\bf h}^{(t)}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\left({\\bf h}^{(t-1)}\\right)^\\top\\\\\n",
    "&\\frac{\\partial \\mathcal{L}}{\\partial {\\bf W}_{{\\bf x}{\\bf h}}}=\\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\frac{\\partial {\\bf h}^{(t)}}{\\partial {\\bf W}_{{\\bf x}{\\bf h}}}=\\sum_{t=1}^{T} \\text{diag}\\left({\\bf 1}-\\left({\\bf h}^{(t)}\\right)^2\\right)\\frac{\\partial \\mathcal{L}}{\\partial {\\bf h}^{(t)}}\\left({\\bf x}^{(t)}\\right)^\\top\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c39445",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f96f40ae-299f-4893-8d16-7822215ce2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "323139b9-fe23-403d-9cb7-f0a8ce38e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    def __init__(self, fpath):\n",
    "        self._fpath = fpath\n",
    "        with open(self._fpath, 'r') as f:\n",
    "            self.data = f.read()\n",
    "            self.data = self.data.replace(\"\\n\", ' ')\n",
    "            self.data = self.data.lower()\n",
    "        self.data = re.sub(r'([\\\"\\'])?([A-z]+[-\\']?[A-z]+)?([\\\"\\':,.?!])?', r'\\1 \\2 \\3', self.data).split()\n",
    "        chars = list(set(self.data))\n",
    "        self.char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "        self.vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e6dcab6e-5bce-4b94-bd63-f013dfee0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size, learning_rate=0.1, sequence_length=25, hidden_size=100):\n",
    "        self._learning_rate = learning_rate # learning rate\n",
    "        self._sequence_length = sequence_length\n",
    "        self._hidden_size = hidden_size\n",
    "        self._vocab_size = vocab_size\n",
    "        np.random.seed(42)\n",
    "        self._Wxh = np.random.randn(self._hidden_size,self._vocab_size)*0.01\n",
    "        self._Whh = np.random.randn(self._hidden_size,self._hidden_size)*0.01\n",
    "        self._Who = np.random.randn(self._vocab_size,self._hidden_size)*0.01\n",
    "        self._bh = np.zeros(hidden_size)\n",
    "        self._bo = np.zeros(vocab_size)\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        ex = np.exp(x-max(x))\n",
    "        return ex/np.sum(ex)\n",
    "        \n",
    "    def loss(self, os, y):\n",
    "        return sum(-np.log(max(os[t][y[t]],0.1/self._vocab_size)) for t in range(self._sequence_length))\n",
    "        # mind the \\\n",
    "        # return loss using Xentropy\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os = {}, {}, {}\n",
    "        hs[-1] = hprev\n",
    "        for t in range(self._sequence_length):\n",
    "            xs[t] = np.zeros(self._vocab_size)\n",
    "            xs[t][inputs[t]] = 1\n",
    "            hs[t] = np.tanh(self._Wxh@xs[t]+self._Whh@hs[t-1]+self._bh)\n",
    "            os[t] = self._softmax(self._Who@hs[t]+self._bo)\n",
    "        return xs, hs, os\n",
    "\n",
    "    \n",
    "    def backward(self, xs, hs, os, y):\n",
    "        dbo, dbh = np.zeros_like(self._bo), np.zeros_like(self._bh)\n",
    "        dWho, dWhh, dWxh = np.zeros_like(self._Who), np.zeros_like(self._Whh), np.zeros_like(self._Wxh)\n",
    "        dhnext = np.zeros(self._hidden_size)\n",
    "        for t in range(self._sequence_length-1,-1,-1):\n",
    "            do = np.zeros(self._vocab_size)+os[t]\n",
    "            do[y[t]] -= 1\n",
    "            dh = (1-hs[t]**2)*(self._Who.T@do+dhnext)\n",
    "            dhnext = self._Whh.T@dh\n",
    "            dbo += do\n",
    "            dbh += dh\n",
    "            dWho += np.outer(do,hs[t])\n",
    "            dWhh += np.outer(dh,hs[t-1])\n",
    "            dWxh += np.outer(dh,xs[t])\n",
    "        for dparam in (dbo, dbh, dWho, dWhh, dWxh):\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dbo, dbh, dWho, dWhh, dWxh\n",
    "    \n",
    "    def fit(self, reader, epoch):\n",
    "        smooth_loss = -np.log(1.0/self._vocab_size)*self._sequence_length\n",
    "        mbo, mbh = np.zeros_like(self._bo), np.zeros_like(self._bh)\n",
    "        mWho, mWhh, mWxh = np.zeros_like(self._Who), np.zeros_like(self._Whh), np.zeros_like(self._Wxh)\n",
    "        \n",
    "        for _ in tqdm(range(epoch)): \n",
    "            n = 0\n",
    "            hprev = np.zeros(self._hidden_size)\n",
    "            for p in range(0, len(reader.data)-self._sequence_length, self._sequence_length):\n",
    "                inputs = [reader.char_to_ix[ch] for ch in reader.data[p:p+self._sequence_length]]\n",
    "                #print(inputs)\n",
    "                y = inputs[1:]+[reader.char_to_ix[reader.data[p+self._sequence_length]]]\n",
    "                #print(y)\n",
    "                xs, hs, os = self.forward(inputs, hprev)\n",
    "                dbo, dbh, dWho, dWhh, dWxh = self.backward(xs, hs, os, y)\n",
    "                #print(dWho)\n",
    "                for param, dparam, mem in zip((self._bo, self._bh, self._Who, self._Whh, self._Wxh),\n",
    "                                              (dbo, dbh, dWho, dWhh, dWxh),\n",
    "                                              (mbo, mbh, mWho, mWhh, mWxh)):\n",
    "                    mem += dparam*dparam\n",
    "                    param += -self._learning_rate*dparam/np.sqrt(mem+1e-8)\n",
    "                hprev = hs[self._sequence_length-1]\n",
    "                smooth_loss = smooth_loss * 0.999 + self.loss(os, y) * 0.001\n",
    "                n+=1\n",
    "                if n%400 == 0:\n",
    "                    print(f\"loss: {smooth_loss}\")\n",
    "                    print(self.predict(reader,'I', self._sequence_length))\n",
    "\n",
    "    def predict(self, reader, start, length):\n",
    "        start = start.lower()\n",
    "        start_ix = reader.char_to_ix[start]\n",
    "        x = np.zeros(self._vocab_size)\n",
    "        x[start_ix] = 1\n",
    "        ixes = [start_ix]\n",
    "        hprev = np.zeros(self._hidden_size)\n",
    "        for t in range(length):\n",
    "            hprev = np.tanh(self._Wxh@x+self._Whh@hprev+self._bh) \n",
    "            o = self._softmax(self._Who@hprev+self._bo)\n",
    "            ix = np.random.choice(range(self._vocab_size), p=o)\n",
    "            ixes.append(ix)\n",
    "            x = np.zeros(self._vocab_size)\n",
    "            x[ix] = 1\n",
    "        text = ' '.join(reader.ix_to_char[ix] for ix in ixes)\n",
    "        text = re.sub(R'([\\\"\\']) ([A-z]+[-\\']?[A-z]+) ([\\\"\\'])', r'\\1\\2\\3', text)\n",
    "        text = re.sub(R'([A-z\\'\\\"]) ([:,.?!])', r'\\1\\2', text)\n",
    "        return text.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c1871825-6e02-40d0-a407-8a6a1f1bbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fpath = 'shakespeare.txt'\n",
    "reader = Reader(fpath)\n",
    "print(f'{reader.vocab_size} unique words')\n",
    "rnn = RNN(vocab_size=reader.vocab_size,hidden_size=500)\n",
    "rnn.fit(reader=reader, epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34deba",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "de3fd328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, vocab_size, hidden_size=100, sequence_length=25, learning_rate=0.01, beta1=0.9, beta2=0.999):\n",
    "        self._hidden_size = hidden_size\n",
    "        self._vocab_size = vocab_size\n",
    "        self._sequence_length = sequence_length \n",
    "        self._learning_rate = learning_rate\n",
    "        self._beta1 = beta1 # momentum parameter in Adam\n",
    "        self._beta2 = beta2 # rmsprop parameter in Adam\n",
    "        self._params = {}\n",
    "        # glorot initialization \n",
    "        std = 1/np.sqrt(self._hidden_size+self._vocab_size)\n",
    "        # forget gate\n",
    "        self._params['Wf'] = np.random.randn(self._hidden_size, self._hidden_size+self._vocab_size) * std\n",
    "        self._params['bf'] = np.zeros((self._hidden_size,1))\n",
    "        # input gate\n",
    "        self._params['Wi'] = np.random.randn(self._hidden_size, self._hidden_size+self._vocab_size) * std\n",
    "        self._params['bi'] = np.zeros((self._hidden_size,1))\n",
    "        # cell gate\n",
    "        self._params['Wc'] = np.random.randn(self._hidden_size, self._hidden_size+self._vocab_size) * std\n",
    "        self._params['bc'] = np.zeros((self._hidden_size,1))\n",
    "        # output gate\n",
    "        self._params['Wo'] = np.random.randn(self._hidden_size, self._hidden_size+self._vocab_size) * std\n",
    "        self._params['bo'] = np.zeros((self._hidden_size,1))\n",
    "        # output\n",
    "        self._params['Wy'] = np.random.randn(self._vocab_size, self._hidden_size) / np.sqrt(self._hidden_size)\n",
    "        self._params['by'] = np.zeros((self._vocab_size,1))\n",
    "    \n",
    "    @property\n",
    "    def _Wf(self): return self._params['Wf']\n",
    "    @property\n",
    "    def _bf(self): return self._params['bf']\n",
    "    @property\n",
    "    def _Wi(self): return self._params['Wi']\n",
    "    @property\n",
    "    def _bi(self): return self._params['bi']\n",
    "    @property\n",
    "    def _Wc(self): return self._params['Wc']\n",
    "    @property\n",
    "    def _bc(self): return self._params['bc']\n",
    "    @property\n",
    "    def _Wo(self): return self._params['Wo']\n",
    "    @property\n",
    "    def _bo(self): return self._params['bo']\n",
    "    @property\n",
    "    def _Wy(self): return self._params['Wy']\n",
    "    @property\n",
    "    def _by(self): return self._params['by']\n",
    "    \n",
    "    def loss(self, Yhat, y):\n",
    "        return sum(-np.log(max((Yhat[t].ravel())[y[t]],0.1/self._vocab_size)) for t in range(self._sequence_length))\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        ex = np.exp(x-max(x))\n",
    "        return ex/np.sum(ex)\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def forward(self, inputs, hprev, cprev):\n",
    "        X, Z, F, I, Cbar, C, O, H, Yhat = {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "        H[-1] = hprev\n",
    "        C[-1] = cprev\n",
    "        for t in range(self._sequence_length):\n",
    "            X[t] = np.zeros((self._vocab_size,1))\n",
    "            X[t][inputs[t]] = 1\n",
    "            Z[t] = np.row_stack((H[-1], X[t]))\n",
    "            F[t] = self._sigmoid(self._Wf@Z[t]+self._bf)\n",
    "            I[t] = self._sigmoid(self._Wi@Z[t]+self._bi)\n",
    "            Cbar[t] = np.tanh(self._Wc@Z[t]+self._bc)\n",
    "            C[t] = F[t]*C[t-1] + I[t]*Cbar[t]\n",
    "            O[t] = self._sigmoid(self._Wo@Z[t]+self._bo)\n",
    "            H[t] = O[t]*np.tanh(C[t])\n",
    "            Yhat[t] = self._softmax(self._Wy@H[t]+self._by)\n",
    "        return Z, F, I, Cbar, C, O, H, Yhat\n",
    "                \n",
    "    def backward(self, Z, F, I, Cbar, C, O, H, Yhat, y):\n",
    "        dparams = {key:np.zeros_like(self._params[key]) for key in self._params.keys()}\n",
    "        dhnext = np.zeros((self._hidden_size,1))\n",
    "        dcnext = np.zeros((self._hidden_size,1))\n",
    "        for t in range(self._sequence_length-1, -1, -1):\n",
    "            dy = np.zeros((self._vocab_size,1))+Yhat[t]\n",
    "            dy[y[t]] -= 1\n",
    "            dh = self._Wy.T@dy + dhnext\n",
    "            dc = dh*O[t]*(1-np.tanh(C[t])**2) +dcnext\n",
    "            # output\n",
    "            dparams['Wy'] += dy@H[t].T\n",
    "            dparams['by'] += dy\n",
    "            # forget gate\n",
    "            df = dc*C[t-1]*(F[t]-F[t]**2)\n",
    "            dparams['Wf'] += df@Z[t].T\n",
    "            dparams['bf'] += df\n",
    "            # input gate\n",
    "            di = dc*Cbar[t]*(I[t]-I[t]**2)\n",
    "            dparams['Wi'] += di@Z[t].T\n",
    "            dparams['bi'] += di\n",
    "            # cell gate\n",
    "            dcbar = dc*I[t]*(1-Cbar[t]**2)\n",
    "            dparams['Wc'] += dcbar@Z[t].T\n",
    "            dparams['bc'] += dcbar\n",
    "            # output gate\n",
    "            do = dh*np.tanh(C[t])*(O[t]-O[t]**2)\n",
    "            dparams['Wo'] += do@Z[t].T\n",
    "            dparams['bo'] += do\n",
    "            # update for next iteration\n",
    "            dhnext = (self._Wf.T@df + self._Wi.T@di + self._Wc.T@dc + self._Wo.T@do)[:self._hidden_size, :]\n",
    "            dcnext = dc*F[t]\n",
    "        for dparam in dparams.values():\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dparams\n",
    "    \n",
    "    def fit(self, reader, epoch, verbose=True):\n",
    "        smooth_loss = -np.log(1.0/self._vocab_size)*self._sequence_length\n",
    "        vparams = {key:np.zeros_like(self._params[key]) for key in self._params.keys()}\n",
    "        sparams = {key:np.zeros_like(self._params[key]) for key in self._params.keys()}\n",
    "        n = 1\n",
    "        for _ in tqdm(range(epoch)):\n",
    "            hprev = np.zeros((self._hidden_size,1))\n",
    "            cprev = np.zeros((self._hidden_size,1))\n",
    "            for p in range(0, len(reader.data)-self._sequence_length, self._sequence_length):\n",
    "                inputs = [reader.char_to_ix[ch] for ch in reader.data[p:p+self._sequence_length]]\n",
    "                y = inputs[1:]+[reader.char_to_ix[reader.data[p+self._sequence_length]]]\n",
    "                if verbose:\n",
    "                    if n%100 == 0:\n",
    "                        print(f\"loss: {smooth_loss}\")\n",
    "                        print(self.predict(reader,reader.data[p], self._sequence_length, hprev, cprev))\n",
    "                Z, F, I, Cbar, C, O, H, Yhat = self.forward(inputs, hprev, cprev)\n",
    "                dparams = self.backward(Z, F, I, Cbar, C, O, H, Yhat, y)\n",
    "                for key in self._params.keys():\n",
    "                    vparams[key] = self._beta1*vparams[key]+(1-self._beta1)*dparams[key]\n",
    "                    sparams[key] = self._beta2*sparams[key]+(1-self._beta2)*dparams[key]**2\n",
    "                    v_corrected = vparams[key]/(1-self._beta1**n)\n",
    "                    s_corrected = sparams[key]/(1-self._beta2**n)\n",
    "                    self._params[key] -= self._learning_rate*v_corrected/(np.sqrt(s_corrected)+1e-8)\n",
    "                hprev = H[self._sequence_length-1]\n",
    "                cprev = C[self._sequence_length-1]\n",
    "                smooth_loss = smooth_loss*0.999 + self.loss(Yhat, y)*0.001\n",
    "                n += 1    \n",
    "                \n",
    "    def predict(self, reader, start, length, hprev=np.array([None]), cprev=np.array([None])):\n",
    "        start = start.lower()\n",
    "        start_ix = reader.char_to_ix[start]\n",
    "        hprev = hprev.copy() if hprev.all() else np.zero((self._hidden_size,1))\n",
    "        cprev = cprev.copy() if cprev.all() else np.zero((self._hidden_size,1))\n",
    "        x = np.zeros((self._vocab_size,1))\n",
    "        x[start_ix] = 1\n",
    "        ixes = [start_ix]\n",
    "        for t in range(length):\n",
    "            z = np.row_stack((hprev, x))\n",
    "            f = self._sigmoid(self._Wf@z+self._bf)\n",
    "            i = self._sigmoid(self._Wi@z+self._bi)\n",
    "            cbar = np.tanh(self._Wc@z+self._bc)\n",
    "            cprev = f*cprev + i*cbar\n",
    "            c = cprev\n",
    "            o = self._sigmoid(self._Wo@z+self._bo)\n",
    "            hprev = o*np.tanh(c)\n",
    "            h = hprev\n",
    "            yhat = self._softmax(self._Wy@h+self._by)\n",
    "            ix = np.random.choice(range(self._vocab_size), p=yhat.ravel())\n",
    "            ixes.append(ix)\n",
    "            x = np.zeros((self._vocab_size,1))\n",
    "            x[ix] = 1\n",
    "        text = ' '.join(reader.ix_to_char[ix] for ix in ixes)\n",
    "        text = re.sub(R'([\\\"\\']) ([A-z]+[-\\']?[A-z]+) ([\\\"\\'])', r'\\1\\2\\3', text)\n",
    "        text = re.sub(R'([A-z\\'\\\"]) ([:,.?!])', r'\\1\\2', text)\n",
    "        return text.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "49fa5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "fpath = 'shakespeare.txt'\n",
    "reader = Reader(fpath)\n",
    "print(f'{reader.vocab_size} unique words')\n",
    "lstm = LSTM(vocab_size=reader.vocab_size)\n",
    "lstm.fit(reader=reader, epoch=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "columbia",
   "language": "python",
   "name": "columbia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
